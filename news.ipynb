{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import gensim.models\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/train_set.csv', sep='\\t')\n",
    "test = pd.read_csv('test/test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1        4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2        7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3        7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4        3646 3055 3055 2490 4659 6065 3370 5814 2465 5...\n",
       "                               ...                        \n",
       "49995    3725 4498 2282 1647 6293 4245 4498 3615 1141 2...\n",
       "49996    4811 465 3800 1394 3038 2376 2327 5165 3070 57...\n",
       "49997    5338 1952 3117 4109 299 6656 6654 3792 6831 21...\n",
       "49998    893 3469 5775 584 2490 4223 6569 6663 2124 168...\n",
       "49999    2400 4409 4412 2210 5122 4464 7186 2465 1327 9...\n",
       "Name: text, Length: 250000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=pd.concat([train['text'],test['text']])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(texts.values, size=63, iter=10,window=12, min_count=1,\n",
    "                                        workers=8,seed=2020,sample=1e-5,sg=1,hs=1)\n",
    "word2vec_model.save('texts.model')\n",
    "print(\"success!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 50000 个耗时： 1665.9355807304382\n",
      "前 100000 个耗时： 3205.9941194057465\n",
      "前 150000 个耗时： 4838.847198486328\n",
      "前 200000 个耗时： 6518.758146047592\n",
      "前 50000 个耗时： 1672.5965094566345\n"
     ]
    }
   ],
   "source": [
    "train['text']\n",
    "def Get_vector(trace):    \n",
    "    word2vec_model = gensim.models.Word2Vec.load('texts.model')\n",
    "    index2word_set = set(word2vec_model.wv.index2word)\n",
    "    startTime = time.time()\n",
    "    vector_list = []\n",
    "    num=0\n",
    "    for cutWords in trace:\n",
    "        i=0\n",
    "        article_vector = np.zeros((word2vec_model.trainables.layer1_size))\n",
    "        for cutword in cutWords:\n",
    "            if cutword in index2word_set:\n",
    "                article_vector = np.add(article_vector, word2vec_model.wv[cutword])\n",
    "                i += 1\n",
    "        cutWord_vector = np.divide(article_vector, i)\n",
    "        vector_list.append(list(cutWord_vector))\n",
    "        del cutWord_vector\n",
    "        del article_vector\n",
    "        num+=1\n",
    "        if num %50000==0:\n",
    "            print(\"前\",num,\"个耗时：\",(time.time() - startTime))\n",
    "    return pd.DataFrame(vector_list)\n",
    "train_vector=Get_vector(train['text'].values)\n",
    "test_vector=Get_vector(test['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector.to_csv(\"train_vec.csv\",index=False)\n",
    "test_vector.to_csv(\"test_vector.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200000x6977 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 56074040 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "\n",
    "word_vectorizer.fit(texts)\n",
    "train_word_features = word_vectorizer.transform(train['text'].values)\n",
    "test_word_features = word_vectorizer.transform(test['text'].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9484625 0.9218392965754442\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(train_word_features,train['label'],test_size=0.2,random_state=1)\n",
    "clf = LogisticRegression(C=4)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "train_scores = clf.score(x_train, y_train)\n",
    "print(train_scores, f1_score(y_pred, y_test, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=clf.predict(test_word_features)\n",
    "label=pd.DataFrame(label,columns=['label'])\n",
    "label.to_csv(\"label.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.754206\tvalid_1's multi_logloss: 1.09841\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's multi_logloss: 0.241205\tvalid_1's multi_logloss: 0.320826\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 6977 and input n_features is 63 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6b294bb68b85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m         result = self.predict_proba(X, raw_score, num_iteration,\n\u001b[1;32m--> 814\u001b[1;33m                                     pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    815\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m         \"\"\"\n\u001b[0;32m    862\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n\u001b[1;32m--> 863\u001b[1;33m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    864\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m                              \u001b[1;34m\"match the input. Model n_features_ is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                              % (self._n_features, n_features))\n\u001b[0m\u001b[0;32m    664\u001b[0m         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n\u001b[0;32m    665\u001b[0m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 6977 and input n_features is 63 "
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "x_train,x_test,y_train,y_test=train_test_split(train_word_features,train['label'],test_size=0.2,random_state=1)\n",
    "\n",
    "clf=lgb.LGBMClassifier(\n",
    "    n_estimators=700,\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    objective='multiclass',\n",
    "    num_class=14)\n",
    "\n",
    "clf.fit(x_train,y_train,\n",
    "         eval_set=[(x_train,y_train),(x_test,y_test)],\n",
    "         verbose=100,\n",
    "         early_stopping_rounds=100)\n",
    "\n",
    "y_pred=clf.predict(x_test)\n",
    "label=clf.predict(test_vector)\n",
    "score=f1_score(y_test, y_pred, average='macro')\n",
    "print(\"score:\",score)\n",
    "\n",
    "label=pd.DataFrame(label,columns=['label'])\n",
    "label.to_csv(\"lgb_label.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
